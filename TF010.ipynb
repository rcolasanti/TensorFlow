{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load large data for network from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 100\n",
    "hm_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "\n",
    "batch_size = 32\n",
    "total_batches = int(1600000/batch_size)\n",
    "hm_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfAnn(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hidden_1_layer = {'weights':[],'biases':[]}\n",
    "        self.hidden_2_layer = {'weights':[],'biases':[]}\n",
    "        self.hidden_3_layer = {'weights':[],'biases':[]}\n",
    "        self.output_layer = {'weights':[],'biases':[]}\n",
    "        self.np_hidden_1_layer={\"weights\":[],\"biases\":[]}\n",
    "        self.np_hidden_2_layer={\"weights\":[],\"biases\":[]}\n",
    "        self.np_hidden_3_layer={\"weights\":[],\"biases\":[]}\n",
    "        self.np_output_layer={\"weights\":[],\"biases\":[]}\n",
    "    \n",
    "    def init_empty(self,size,n_nodes_hl1,n_nodes_hl2,n_nodes_hl3,n_classes):\n",
    "        self.hidden_1_layer = {'weights':tf.Variable(tf.random_normal([size, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "        self.hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                          'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "        self.hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                          'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "        self.output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                        'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "\n",
    "    def init_values(self,size,l1_weights,l2_weights,l3_weights,out_weights,l1_biases,l2_biases,l3_biases,out_biases):\n",
    "        self.hidden_1_layer = {'weights':tf.Variable(l1_weights),'biases':tf.Variable(l1_biases)}\n",
    "        self.hidden_2_layer = {'weights':tf.Variable(l2_weights),'biases':tf.Variable(l2_biases)}\n",
    "        self.hidden_3_layer = {'weights':tf.Variable(l3_weights),'biases':tf.Variable(l3_biases)}\n",
    "        self.output_layer = {'weights':tf.Variable(out_weights),'biases':tf.Variable(out_biases)}\n",
    "\n",
    "\n",
    "    def create(self,data):\n",
    "        # This is the heart of the ann where multiply the data by the wights to the layers \n",
    "        l1 = tf.add(tf.matmul(data,self.hidden_1_layer['weights']), self.hidden_1_layer['biases'])\n",
    "        l1 = tf.nn.relu(l1)\n",
    "\n",
    "        l2 = tf.add(tf.matmul(l1,self.hidden_2_layer['weights']), self.hidden_2_layer['biases'])\n",
    "        l2 = tf.nn.relu(l2)\n",
    "\n",
    "        l3 = tf.add(tf.matmul(l2,self.hidden_3_layer['weights']), self.hidden_3_layer['biases'])\n",
    "        l3 = tf.nn.relu(l3)\n",
    "\n",
    "        output =  tf.add(tf.matmul(l3,self.output_layer['weights']) , self.output_layer['biases'])\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def extract(self):\n",
    "        self.np_hidden_1_layer[\"weights\"] = neural_network_model.hidden_1_layer[\"weights\"].eval()\n",
    "        self.np_hidden_2_layer[\"weights\"] = neural_network_model.hidden_2_layer[\"weights\"].eval()\n",
    "        self.np_hidden_3_layer[\"weights\"] = neural_network_model.hidden_3_layer[\"weights\"].eval()\n",
    "        self.np_output_layer[\"weights\"] = neural_network_model.output_layer[\"weights\"].eval()\n",
    "        self.np_hidden_1_layer[\"biases\"] = neural_network_model.hidden_1_layer[\"biases\"].eval()\n",
    "        self.np_hidden_2_layer[\"biases\"] = neural_network_model.hidden_2_layer[\"biases\"].eval()\n",
    "        self.np_hidden_3_layer[\"biases\"] = neural_network_model.hidden_3_layer[\"biases\"].eval()\n",
    "        self.np_output_layer[\"biases\"] = neural_network_model.output_layer[\"biases\"].eval()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_interactive(neural_network_model):\n",
    "    x= tf.placeholder('float')\n",
    "    y = tf.placeholder('float')\n",
    "    prediction = neural_network_model.create(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    hm_epochs = 10\n",
    "    sess = tf.InteractiveSession()\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    epoch = 1\n",
    "    while epoch <= hm_epochs:\n",
    "        epoch_loss = 0\n",
    "        with open('Data/lexicon-2500-2638.pickle','rb') as f:\n",
    "            lexicon = pickle.load(f)\n",
    "        with open('Data/train_set_shuffled008.csv', buffering=20000, encoding='latin-1') as f:\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batches_run = 0\n",
    "            for line in f:\n",
    "                label = line.split(':::')[0]\n",
    "                tweet = line.split(':::')[1]\n",
    "                current_words = word_tokenize(tweet.lower())\n",
    "                current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "\n",
    "                features = np.zeros(len(lexicon))\n",
    "\n",
    "                for word in current_words:\n",
    "                    if word.lower() in lexicon:\n",
    "                        index_value = lexicon.index(word.lower())\n",
    "                        # OR DO +=1, test both\n",
    "                        features[index_value] += 1\n",
    "                line_x = list(features)\n",
    "                line_y = eval(label)\n",
    "                batch_x.append(line_x)\n",
    "                batch_y.append(line_y)\n",
    "                if len(batch_x) >= batch_size:\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: np.array(batch_x),\n",
    "                                                              y: np.array(batch_y)})\n",
    "                    epoch_loss += c\n",
    "                    batch_x = []\n",
    "                    batch_y = []\n",
    "                    batches_run +=1\n",
    "                    if batches_run%10000==0:\n",
    "                        print('Batch run:',batches_run,'/',total_batches,'| Epoch:',epoch,'| Batch Loss:',c,)\n",
    "                    if batches_run%1000==0:\n",
    "                        print(batches_run,end=\":\")\n",
    "                        print(epoch,end=\" \")\n",
    "\n",
    "    neural_network_model.extract()\n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    print('Accuracy:',accuracy.eval({x:test_x, y:test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000:1 2000:1 3000:1 4000:1 5000:1 6000:1 7000:1 8000:1 9000:1 Batch run: 10000 / 50000 | Epoch: 1 | Batch Loss: 156.84756\n",
      "10000:1 11000:1 12000:1 13000:1 14000:1 15000:1 16000:1 17000:1 18000:1 19000:1 Batch run: 20000 / 50000 | Epoch: 1 | Batch Loss: 17.1883\n",
      "20000:1 21000:1 22000:1 23000:1 24000:1 25000:1 26000:1 27000:1 28000:1 29000:1 Batch run: 30000 / 50000 | Epoch: 1 | Batch Loss: 10.417189\n",
      "30000:1 31000:1 32000:1 33000:1 34000:1 35000:1 36000:1 37000:1 38000:1 39000:1 Batch run: 40000 / 50000 | Epoch: 1 | Batch Loss: 2.8684473\n",
      "40000:1 41000:1 42000:1 43000:1 44000:1 45000:1 46000:1 47000:1 48000:1 49000:1 Batch run: 50000 / 50000 | Epoch: 1 | Batch Loss: 2.5914087\n",
      "50000:1 51000:1 52000:1 53000:1 54000:1 55000:1 56000:1 57000:1 58000:1 59000:1 Batch run: 60000 / 50000 | Epoch: 1 | Batch Loss: 2.7381139\n",
      "60000:1 61000:1 62000:1 63000:1 64000:1 65000:1 66000:1 67000:1 68000:1 69000:1 Batch run: 70000 / 50000 | Epoch: 1 | Batch Loss: 3.6663966\n",
      "70000:1 71000:1 72000:1 73000:1 74000:1 75000:1 76000:1 77000:1 78000:1 79000:1 Batch run: 80000 / 50000 | Epoch: 1 | Batch Loss: 0.6237983\n",
      "80000:1 81000:1 82000:1 83000:1 84000:1 85000:1 86000:1 87000:1 88000:1 89000:1 Batch run: 90000 / 50000 | Epoch: 1 | Batch Loss: 0.73969567\n",
      "90000:1 91000:1 92000:1 93000:1 94000:1 "
     ]
    }
   ],
   "source": [
    "neural_network_model = TfAnn()\n",
    "neural_network_model.init_empty(2638,n_nodes_hl1,n_nodes_hl2,n_nodes_hl3,n_classes)\n",
    "train_neural_network_interactive(neural_network_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
